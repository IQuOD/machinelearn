######################################################################################################
######################################################################################################

# big loop to find optimal values for stuff (starting with temperature increases)
def get_stats(data, consec_points):
	
	# determining the depth of actual HB flag:
	for i in range(0,len(flags.flag)):
			if (flags.flag[i] == "HB"):
				HB_depth = flags.depth[i]
			else:
 				continue
	
	try:
		# calculate peak accuracy (spike function returns int 0 if no peak found)
		temp_peak = temp_increase(data, consec_points) 
		temp_depth = temp_peak[0][0]
		difference = HB_depth - temp_depth
	
		# categorising (1=good, 0=bad)
		if (abs(difference) < 5):
			temp_stats = 1
		else:
			temp_stats = 0
	except:
		temp_stats = 0
		pass

	return(temp_stats)

# big loop to find optimal values for stuff (starting with temperature increases)
def get_stats_all(data, consec_points):
	
	# determining the depth of actual HB flag:
	for i in range(0,len(flags.flag)):
			if (flags.flag[i] == "HB"):
				HB_depth = flags.depth[i]
			else:
 				continue
	
	try:
		# calculate peak accuracy (spike function returns int 0 if no peak found)
		peaks = temp_increase(data, consec_points) 
		count = 0
	
		# categorising (1=good, 0=bad)
		for j in range(0,len(peaks)):
			difference = peaks[j][0] - HB_depth
			if (abs(difference) < 5):
				count = count + 1
	except:
		count = 0
		pass

	return(count)

# for loop to go over all data
consec_points = np.arange(10,160,10)
t_consec = []
tall_consec = []

for j in range(0,len(consec_points)):
	print(j)
	total_count = 0 
	total_all_counts = 0
	for i in range(0,len(name_array)):
		print(i)
	
		# reading in file here
		filename = name_array[i]
		read_data(filename)
		total_count = total_count + get_stats(data, consec_points[j])
		total_all_counts = total_all_counts + get_stats_all(data, consec_points[j])
		
	# store in bigger array to know which is best
	t_consec.append(total_count)
	tall_consec.append(total_all_counts)

# printing overall counts per number of consecutive points
print(t_consec)
print(tall_consec)

######################################################################################################
######################################################################################################

def get_stats(data, gradient, consec_points, detection_threshold):
	
	# determining the depth of actual HB flag:
	for i in range(0,len(flags.flag)):
			if (flags.flag[i] == "HB"):
				HB_depth = flags.depth[i]
			else:
 				continue
	
	try:
		# calculate peak accuracy (spike function returns int 0 if no peak found)
		peak = const_temp(data, gradient, consec_points, detection_threshold)
		temp_stats = 0	
	
		for j in range(0,len(peak)):
			difference = peak[j][0] - HB_depth
			if (abs(difference) < 5):
				temp_stats = temp_stats + 1
	except:
		temp_stats = 0
		pass

	return(temp_stats)

# for loop to go over all data
detection_threshold = np.logspace(-8,-1,8)
all_count = []

for j in range(0,len(detection_threshold)):
	print(j)
	total_count = 0 
	for i in range(0,len(name_array)):
		print(i)
	
		# reading in file here
		filename = name_array[i]
		read_data(filename)
		total_count = total_count + get_stats(data, gradient, 100, detection_threshold[j])
		
	# store in bigger array to know which is best
	all_count.append(total_count)

# printing overall counts per number of consecutive points
print(all_count)



######################################################################################################
######################################################################################################

# plotting additonal features
		consecpts = const_temp(data,gradient,100,0.001)
		plt.plot(consecpts[:,1],consecpts[:,0],'go')
		grow = temp_increase(data,50)
		plt.plot(grow[:,1],grow[:,0],'bo')	
		bath_z = bath_depth(latitude, longitude, bath_lon, bath_lat, bath_height)
		plt.axhline(y=bath_z, hold=None, color='g')	
		spikes = grad_spike(data,gradient, 3)
		if (type(spikes) != int):
			plt.plot(spikes[:,1], spikes[:,0],'ro')
		small_spikes = T_spike(data, 0.05)
		plt.plot(small_spikes[:,1],small_spikes[:,0],'yo')		


######################################################################################################
######################################################################################################

	# looking at the second derivative
	d2Tdz2 = []
	depth_secgrad = []
	
	for jj in range(0,n-2):
		depth_secgrad.append(gradient[jj][0])
		der = float((gradient[jj+1][1]-gradient[jj][1])/(gradient[jj+1][1]-gradient[jj][1]))
		d2Tdz2.append(der)
	secDer = np.column_stack((depth_secgrad,d2Tdz2))

	# taking a moving average for the temperature values
	depth_9pt = []
	temp9pt = []
	n = len(gradient[:,0])
	for jj in range(4,n-4):
		depth_9pt.append(gradient[jj][0])
		Tav = (gradient[jj-4][1]+gradient[jj-3][1]+gradient[jj-2][1]+gradient[jj-1][1]+gradient[jj][1]+gradient[jj+1][1]+gradient[jj+2][1]+gradient[jj+3][1]+gradient[jj+4][1])/float(9.0)
		temp9pt.append(Tav)
	dT9pt = np.column_stack((depth_9pt,temp9pt))


######################################################################################################
######################################################################################################

		# plotting temperature 9 point moving average
		plt.subplot(1,3,3)
		plt.plot(dT9pt[:,1], dT9pt[:,0])
		plt.ylabel("Depth [m]")
		plt.xlabel("T - 9pt moving av [degrees C]")
		plt.gca().invert_yaxis()
		plt.title("T 9pt MA")
		for i in range(0,len(flags.flag)):
			if (flags.flag[i] == "HB"):
				ref = flags.depth[i]
				plt.axhline(y=ref, hold=None, color='r')
			else:
 				continue

######################################################################################################
######################################################################################################
"""
This is a section of code that is removed from the hitbottom.py file after being used
"""

# statistics for the spikes (independent)
"""
collect information about what is the best standard deviation threshold (function input) 
to use to get the fewest false detections
The match precision for a "good match" will be set to +- 5m in depth (chosen arbitrarily)
Bad is if no match is detected or if difference is more than 5m
"""
"""
def get_stats(data, gradient, threshold):
	
	# determining the depth of actual HB flag:
	for i in range(0,len(flags.flag)):
			if (flags.flag[i] == "HB"):
				HB_depth = flags.depth[i]
			else:
 				continue
	
	# calculate peak accuracy (spike function returns int 0 if no peak found)
	spikes = spike(data, gradient, threshold) 
	if (type(spikes) != int):
		spike_depth = spikes[0][0]
		difference = HB_depth - spike_depth
	else:
		difference = 999
	
	# categorising (1=good, 0=bad)
	if (abs(difference) < 5):
		spike_stats = 1
	else:
		spike_stats = 0

	return(spike_stats)
"""

# importing libraries
import numpy as np

""" 
Opening file for reading the fraction of good detections found with each threshold
"""

threshold = []
good = []
bad = []

# reading file
with open("stats.txt") as f:
	next(f) #skipping header
	for line in f:
		line = line.split(',')
		threshold.append(int(line[0]))
		good.append(float(line[1]))
		bad.append(float(line[2].rstrip('\n')))

f.close()

threshold = np.array(threshold)
good = np.array(good)
bad = np.array(bad)

# calculation of the fraction of the hit bottoms that are being identified accurately with the
# temperature spikes method
frac = good/(good+bad)

# writing to terminal
for i in range(0,len(threshold)):
	print("for "+str(threshold[i])+"*sigma threshold, HB identified = "+str(frac[i])+" accuracy.")

"""
Find that the 3 sigma threshold for detections is the most ideal for determining HB
"""


######################################################################################################
######################################################################################################



######################################################################################################
######################################################################################################



######################################################################################################
######################################################################################################



######################################################################################################
######################################################################################################
